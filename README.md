# k8s_inference_scheduler

reference papers

**Clipper**

Danieal Crankshaw, Xin Wang, Guilio Zhou, Michael J. Franklin, Joseph E. Gonzalez, and Ion Stoica, "Clipper: A low-latency online prediction serving system," in _Proc. USENIX NSDI 2017_.

**Model-Switching**

Jeff Zhang, Sameh Elnikety, Shuayb Zarar, Atul Gupta, and Siddharth Garg, "Model-switching: Dealing with fluctuating workloads in machine-learning-as-a-service systems," in _Proc. HotCloud 2020_.

**Cocktail**

Jashwant Raj Gunasekaran, Cyan Subhra Mmishra, Prashanth  Thinakaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R. Das, "Cocktail: A multidimensional optimization for model serving in cloud," in _Proc. USENIX NSDI 2022_.

**INFaaS**

Francisco Romero, Qian Li, Neeraja J. Yadwadkar, and Christos Kozyrakis, "INFaaS: Automated model-less inference serving," in _Proc. USENIX ATC 2021_.

Neeraja J. Yadwadkar, Francisco Romero, Qian Li, and Christos Kozyrakis, "A case for managed and model-less inference serving," in _Proc. HotOS 2019_.

**Tolerance Tiers**

Matthew Halpern, Behzad Boroujerdian, Todd Mummert, Evelyn Duesterwald, and Vijay Janapa Reddi, "One size does not fit all: Quntifying and exposing the accuracy-latency trade-off in machine learning cloud service APIs via tolerance tiers," in _Proc. IEEE ISPASS 2019_.

**Abacus**

Weihao Cui, Han Zhao, Quan Chen, Ningxin Zheng, Jingwen Leng, Jieru Zho, Zhuo Song, Tao Ma, Yong Yang, Chao Li, and Minyi Guo, "Enable simultaneous DNN services based on deterministic perator overlkap and precise latency prediction," in _Proc. ACM SC 2021_.

**AlpaServe**

Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica, "AlpaServe: Statistical multiplexing with model parallelism for deep learning serving," in _Proc. USENIX OSDI 2023_.

**DeepPlan**

Jinwoo Jeong, Seungsu Baek, and Jeongseob Ahn, "Fast and efficient model serving using multi-GPUs with direct-host-access," in _Proc. EuroSys 2023_.

**Knowledge Distillation**

Geoffrey Hinton, Oriol Vinyals, and Jeff Dean, "Distilling the Knowledge in a Neural Network," in _Proc. NIPS, Deep Learning Workshop, 2014_.

=implementation links= 

https://github.com/peterliht/knowledge-distillation-pytorch (pytorch)

https://github.com/DushyantaDhyani/kdtf (tensorflow)

**GMorph**

Qizheng Yang, Tianyi Yang, Mingcan Xiang, Lijun Zhang, Haoliang Wang, Marco Serafini, and Hui Guan, "GMorph: Accelerating Multi-DNN Inference via Model Fusion," in _Proc. EuroSys 2024_.

**Proteus**

Sohaib Ahmad, Hui Guan, Brian D. Fiedman, Thomas Williams, Ramesh K. Sitaraman, and Thomas Woo, "Proteus: A High-Thoughput Inference-Serving System with Accuracy Scaling," in _Proc. ACM ASPLOS 2024_.

**Model Selection**

Daniel Mendoza, Francisco Romero, and Caroline Trippel, "Model Selection for Latency-Critical Inference Serving," in _Proc. EuroSys 2024_. 

**Sommelier**

Peizhen Guo, Bo Hu, and Wenjun Hu, "Sommelier: Curating DNN Models for the Masses," in _Proc. SIGMOD 2022_.

**PRETZEL**

Yunseong Lee, Alberto Scolari, Byung-Gon Chu, Marco Domenico Santambrogio, Markus Weimer, and Matteo Interlandi, "PRETZEL: Opening the Black Box of Machine Learning Prediction Serving Systems," in _Proc. USENIX OSDI 2018_.

**VELTAIR**

Zihan Liu, Jingwen Leng, Zhihui Zhang, Quan Chen, Chao Li, and Minyi Guo, "VELTAIR: Towards High-Performance Multi-Tenant Deep Learning Services via Adaptive Compliation and Scheduling," in _Proc. ACM ASPLOS 2022_.

**HuggingGPT**

Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang, "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face," in _Proc. NeurIPS 2023_.

**Tree of Thoughts**

Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan, "Tree of Thoughts: Deliberate Problem Solving with Large Language Models," in _Proc. NeurIPS 2023_.

**Cocktail**

Jashwant Raj Gunasekaran, Cyan Subhra Mmishra, Prashanth  Thinakaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R. Das, "Cocktail: A multidimensional optimization for model serving in cloud," in _Proc. USENIX NSDI 2022_.

**Self Consistency with Chain of Thought**

X. Wnag, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou, "Self-consistency improves chain of thought reasoning in language models," _arXiv preprint arXiv:2203.11171_, 2022.

**MOSEL**

Bodun Hu, Le Xu, Jeongyoon Moon, Neeraja J. Yadwadkar, and Aditya Akella, "MOSEL: Inference Serving Using Dynamic Modality Selection," _arXiv preprint arXiv:2310.18481_, 2023.

**TVLT**

Zineng Tang, Jaemin Cho, Yixin Nie, and Mohit Bansal, "TVLT: Textless vision-language transformer," in _Proc. NeurIPS 2022_.


Tianxing Li, Jin Huang, Erik Risinger, and Deepak Ganesan, "Low-latency speculative inference on distributed multi-modal data streams," in _Proc. ACM MobiSys 2021_.







