# k8s_inference_scheduler

reference papers

**Clipper**

Danieal Crankshaw, Xin Wang, Guilio Zhou, Michael J. Franklin, Joseph E. Gonzalez, and Ion Stoica, "Clipper: A low-latency online prediction serving system," in Proc. USENIX NSDI 2017.

**Tolerance Tiers**

Matthew Halpern, Behzad Boroujerdian, Todd Mummert, Evelyn Duesterwald, and Vijay Janapa Reddi, "One size does not fit all: Quntifying and exposing the accuracy-latency trade-off in machine learning cloud service APIs via tolerance tiers," in Proc. IEEE ISPASS 2019.

**Abacus**

Weihao Cui, Han Zhao, Quan Chen, Ningxin Zheng, Jingwen Leng, Jieru Zho, Zhuo Song, Tao Ma, Yong Yang, Chao Li, and Minyi Guo, "Enable simultaneous DNN services based on deterministic perator overlkap and precise latency prediction," in Proc. SC 2021.

**Cocktail**

Jashwant Raj Gunasekaran, Cyan Subhra Mmishra, Prashanth  Thinakaran, Bikash Sharma, Mahmut Taylan Kandemir, and Chita R. Das, "Cocktail: A multidimensional optimization for model serving in cloud," in Proc. USENIX NSDI 2022.

**AlpaServe**

Zhuohan Li, Lianmin Zheng, Yinmin Zhong, Vincent Liu, Ying Sheng, Xin Jin, Yanping Huang, Zhifeng Chen, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica, "AlpaServe: Statistical multiplexing with model parallelism for deep learning serving," in Proc. OSDI 2023.

**DeepPlan**

Jinwoo Jeong, Seungsu Baek, and Jeongseob Ahn, "Fast and efficient model serving using multi-GPUs with direct-host-access," in Proc. EuroSys 2023.






